{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6fc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/santhise001/projects/sc4001_assignment_ay26_s1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tests import test_category_embedder, test_house_price_regressor, print_dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1b523",
   "metadata": {},
   "source": [
    "B1. Data\n",
    "---\n",
    "\n",
    "a. Load the dataset using pandas\n",
    "\n",
    "b. Create train, test and validation splits\n",
    "\n",
    "c. Encode categorical features\n",
    "\n",
    "d. Scale continuous features\n",
    "\n",
    "e. Implement a PyTorch dataset\n",
    "\n",
    "f. Create PyTorch dataloaders\n",
    "\n",
    "### B1(a) Load the data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20aa1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = ... TODO\n",
    "\n",
    "# define the continuous and categorical columns\n",
    "continuous_cols = [\n",
    "    \"dist_to_nearest_stn\",\n",
    "    \"dist_to_dhoby\",\n",
    "    \"degree_centrality\",\n",
    "    \"eigenvector_centrality\",\n",
    "    \"remaining_lease_years\",\n",
    "    \"floor_area_sqm\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\"month\", \"town\", \"flat_model_type\", \"storey_range\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b5439",
   "metadata": {},
   "source": [
    "### B1(b) Temporal split with validation\n",
    "\n",
    "Create a temporal split:\n",
    "\n",
    "1.\tTrain/Val pool: all rows with year <= 2020.\n",
    "\n",
    "2.\tFrom this pool, randomly sample 10% of the dataset to create a validation set.\n",
    "\n",
    "3.\tTest set: all rows with year == 2021.\n",
    "\n",
    "4.\tPrint shapes (n_rows, n_cols) for train_data, val_data, test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b326fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_data, test_data = ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02614be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc26526",
   "metadata": {},
   "source": [
    "### B1(c) Encode categorical features w/o leakage\n",
    "\n",
    "1.\tCreate an `sklearn.preprocessing.OrdinalEncoder` with handle_unknown=\"use_encoded_value\", unknown_value=-1.\n",
    "\n",
    "2.\tFit on train_data[categorical_cols] only.\n",
    "\n",
    "3.\tTransform val_data and test_data using the fitted encoder.\n",
    "\n",
    "4. Compute and print the cardinality (number of distinct codes) for each categorical column on train_data after encoding. (These will be used to define embedding tables later.)\n",
    "\n",
    "### B1(d) Scale continuous features w/o leakage\n",
    "\n",
    "1.\tCreate a `StandardScaler`.\n",
    "\n",
    "2.\tFit on train_data[continuous_cols] only.\n",
    "\n",
    "3.\tTransform val_data and test_data.\n",
    "\n",
    "4.\tReport the mean and std of each continuous feature on the transformed train split (they should be ~0 and ~1).\n",
    "\n",
    "Do not scale resale_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e871bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd9bbc",
   "metadata": {},
   "source": [
    "### B1(e) Implement a PyTorch Dataset\n",
    "\n",
    "Implement HDBPriceDataset(Dataset) with:\n",
    "- `__init__(self, data, categorical_cols, continuous_cols, target_col=\"resale_price\")`\n",
    "\n",
    "- `__getitem__(idx)` returns a dict: {\"x_cat\": LongTensor, \"x_cont\": FloatTensor, \"y\": FloatTensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# class HDBPriceDataset(Dataset):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eddbbf",
   "metadata": {},
   "source": [
    "### B1(f) Create train, validation and test data loaders\n",
    "\n",
    "Create train, validation and test datasets and data loaders. Use a batch size of 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12b881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = ...\n",
    "\n",
    "# train_dataloader, val_dataloader, test_dataloader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001039fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "print_dataset_info(train_dataset, test_dataset, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7c3cb",
   "metadata": {},
   "source": [
    "B2. Model and training\n",
    "---\n",
    "a. Create a PyTorch Model\n",
    "\n",
    "b. Write a LightningModule for fitting the model\n",
    "\n",
    "c. Train the model\n",
    "\n",
    "d. Predict using the trained model on the test set and calculate $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740b05e",
   "metadata": {},
   "source": [
    "### B2(a) Create a PyTorch Model\n",
    "\n",
    "In this part you will build a PyTorch model for tabular house-price regression that (i) embeds each categorical feature into a dense vector, (ii) concatenates all category embeddings with the standardized continuous features, and (iii) predicts resale_price via a small MLP.\n",
    "\n",
    "You must implement two modules:\n",
    "1.\tCategoryEmbedder — one embedding table per categorical column plus optional dropout.\n",
    "\n",
    "2.\tHousePriceRegressor — uses CategoryEmbedder, concatenates with continuous features, then a 3-layer MLP.\n",
    "\n",
    "You will then instantiate the model using provided cardinalities and embedding dimensions.\n",
    "\n",
    "Use the same feature schema and encoded/scaled splits you produced in Part 1. Assume you already have train_loader, val_loader, test_loader, continuous_cols, oe (the fitted OrdinalEncoder), and batch_size.\n",
    "\n",
    "A. Implement CategoryEmbedder (10 pts)\n",
    "\n",
    "Create a module that receives:\n",
    "- cardinalities: List[int] — number of distinct codes per categorical column (from oe.categories_).\n",
    "\n",
    "- embed_dims: List[int] — embedding dimension for each categorical column (same length as cardinalities).\n",
    "\n",
    "- emb_dropout: float — dropout applied after concatenation of all embeddings.\n",
    "\n",
    "Requirements\n",
    "\n",
    "1.\tConstruct a nn.ModuleList of nn.Embedding(num_embeddings=c, embedding_dim=d) for each (c, d) pair.\n",
    "\n",
    "2.\tIn forward(x_cat), where x_cat has shape [B, C] (C = #categorical columns):\n",
    "\n",
    "\t- Look up each column i using its table: emb_i(x_cat[:, i]) → [B, d_i].\n",
    "\n",
    "\t- Concatenate along feature dimension → [B, sum(embed_dims)].\n",
    "\n",
    "\t- Apply Dropout(emb_dropout) and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CategoryEmbedder(nn.Module):\n",
    "    def __init__(self, cardinalities, embed_dims, embed_dropout=0.0):\n",
    "        \"\"\"Initialize the CategoryEmbedder.\n",
    "        Args:\n",
    "            cardinalities (list): List of integers representing the number of unique values for each categorical feature.\n",
    "            embed_dims (list): List of integers representing the embedding dimensions for each categorical feature.\n",
    "            embed_dropout (float): Dropout rate for the embeddings.\n",
    "        \"\"\"\n",
    "#    ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd926f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_category_embedder(CategoryEmbedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePriceRegressor(nn.Module):\n",
    "    def __init__(self, cardinalities, embed_dims, n_continuous, emb_dropout=0.0):\n",
    "        \"\"\"Initialize the HousePriceRegressor.\n",
    "\n",
    "        Args:\n",
    "            cardinalities (list): List of integers representing the number of unique values for each categorical feature.\n",
    "            embed_dims (list): List of integers representing the embedding dimensions for each categorical feature.\n",
    "            n_continuous (int): Number of continuous features.\n",
    "            emb_dropout (float, optional): Dropout rate for the embeddings. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "    #    ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "test_house_price_regressor(HousePriceRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "cardinalities = [len(categories) for categories in oe.categories_]\n",
    "embedding_dims = [min(50, (c + 1) // 2) for c in cardinalities]\n",
    "n_continuous = len(continuous_cols)\n",
    "\n",
    "# Create model with proper cardinalities\n",
    "model = HousePriceRegressor(\n",
    "    cardinalities=cardinalities,\n",
    "    embed_dims=embedding_dims,\n",
    "    n_continuous=n_continuous,\n",
    "    emb_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea147f",
   "metadata": {},
   "source": [
    "### B2(b) PyTorch LightningModule\n",
    "\n",
    "Wrap your HousePriceRegressor into a PyTorch LightningModule so that training, validation, and testing are managed automatically.\n",
    "\n",
    "What to do\n",
    "\n",
    "1.\tInitialize the module\n",
    "    - Store hyperparameters (embedding sizes, number of continuous features, learning rate, etc.).\n",
    "\n",
    "    - Create an instance of your HousePriceRegressor.\n",
    "\n",
    "    - Choose a suitable regression loss function (e.g., MSE).\n",
    "\n",
    "    - Set up at least one evaluation metric (e.g., MAE, RMSE).\n",
    "\n",
    "2.\tForward method\n",
    "    - Define a forward pass that takes categorical + continuous features and returns predictions.\n",
    "\n",
    "3.\tTraining, Validation, and Test steps\n",
    "    - In each step:\n",
    "        - Extract features and target from the batch.\n",
    "\n",
    "        - Run the model forward.\n",
    "\n",
    "        - Compute the loss.\n",
    "\n",
    "        - Log the loss and chosen metrics so they can be monitored.\n",
    "\n",
    "4.\tPrediction step\n",
    "    - Implement a method to return predictions on new data without computing loss.\n",
    "\n",
    "5.\tOptimiser\n",
    "    - Use a suitable optimiser (e.g., Adam) with a reasonable learning rate.\n",
    "\n",
    "    - (Optional: add a scheduler if you wish to experiment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDBPriceLightningModule(L.LightningModule):\n",
    "    # ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5854175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "lightning_model = HDBPriceLightningModule(\n",
    "    cardinalities=cardinalities,\n",
    "    embedding_dims=embedding_dims,\n",
    "    n_continuous=n_continuous,\n",
    "    emb_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f3ae2",
   "metadata": {},
   "source": [
    "### B2(c) Train the model\n",
    "\n",
    "What to do\n",
    "1.\tCheckpoints\n",
    "    - Add a checkpoint callback that monitors validation loss and keeps the best models (e.g., top-k) plus the last checkpoint for resuming.\n",
    "\n",
    "2.\tEarly stopping\n",
    "    - Add early stopping on the same validation metric with a reasonable patience and a small min_delta (use a tiny positive value to require real improvement).\n",
    "\n",
    "3.\tLearning-rate tracking\n",
    "    - Log the learning rate each epoch/step with a LR monitor so it’s visible in your logger.\n",
    "\n",
    "4.\tExperiment logging\n",
    "    - Create a logger (e.g., TensorBoard) with a clear experiment name and save directory.\n",
    "\n",
    "5.\tTrainer configuration\n",
    "    - Set a sensible number of epochs, devices/accelerator (CPU/GPU), and enable gradient clipping.\n",
    "\n",
    "    - Turn on progress bar and model summary for visibility.\n",
    "\n",
    "    - Choose an appropriate logging frequency for your dataset size.\n",
    "\n",
    "6.\tFit & Test\n",
    "- Call trainer.fit(model, train_loader, val_loader).\n",
    "\n",
    "- Optionally run trainer.test(model, test_loader) after training and print metrics.\n",
    "\n",
    "7. Download the plots for validation loss, your validation metric and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9971f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4fa2d",
   "metadata": {},
   "source": [
    "### B2(d) Model prediction and evaluation\n",
    "\n",
    "1.\tRun prediction\n",
    "    - Use trainer.predict(model, test_loader) to generate predictions on the held-out test set.\n",
    "\n",
    "    - Concatenate predictions into a single tensor/array.\n",
    "\n",
    "2. Report results\n",
    "    - Print R² (rounded to 4 decimal places).\n",
    "\n",
    "    - Interpret the value of R² briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ee0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = ...\n",
    "\n",
    "\n",
    "# print(f\"R^2 on test set =\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177c8dc",
   "metadata": {},
   "source": [
    "B3. Model Interpretability with Integrated Gradients\n",
    "---\n",
    "\n",
    " What to do\n",
    "1.\tChoose an explainability method\n",
    "\n",
    "    - Use Integrated Gradients (IG) from captum.attr.\n",
    "\n",
    "    - Treat the continuous features (x_cont) as the primary inputs for attribution.\n",
    "\n",
    "    - Pass categorical features (x_cat) as additional arguments to the forward function.\n",
    "\n",
    "2.\tDefine a forward wrapper\n",
    "\n",
    "    - Implement a wrapper function so IG knows how to call your model with both categorical and continuous features.\n",
    "\n",
    "3.\tLocal explanation (per sample)\n",
    "\n",
    "    - Run IG on a single batch from the training set.\n",
    "\n",
    "    - Use a zero baseline for continuous features.\n",
    "\n",
    "    - Plot the continuous features ranked by signed contribution for one example.\n",
    "\n",
    "4.\tGlobal explanation (across dataset)\n",
    "\n",
    "    - Loop over the validation set to compute mean absolute attributions for each continuous feature.\n",
    "\n",
    "    - Normalize by the number of samples.\n",
    "\n",
    "    - Rank features by importance and plot a bar chart showing the most influential features globally.\n",
    "\n",
    "5.\tInterpret results\n",
    "\n",
    "    - Report which features matter most for the model’s predictions.\n",
    "\n",
    "    - Reflect briefly: Do these align with intuition (e.g., distance to station, floor area)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ece0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# TODO ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
