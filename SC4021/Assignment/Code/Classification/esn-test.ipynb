{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8062137,"sourceType":"datasetVersion","datasetId":4703032},{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"mport pandas as pd\nimport string\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, svm\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport nltk\nimport subprocess\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport time\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n# metric (AUC, ROC, sensitivity & specificity)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import DataLoader, TensorDataset\n!pip install snntorch\nimport snntorch as snn\nfrom snntorch import surrogate\nfrom snntorch import utils\nimport snntorch.functional as SF\nimport math\n!pip install ncps\n\nimport numpy as np\nimport torch.nn as nn\nfrom ncps.wirings import AutoNCP\nfrom ncps.torch import LTC\nimport pytorch_lightning as pl\nimport torch.utils.data as data\ntorch.manual_seed(42)\n# nltk.download('wordnet')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('stopwords')\n# nltk.download(\"punkt\")\n\n# try:\n#     nltk.data.find('wordnet.zip')\n# except:\n#     nltk.download('wordnet', download_dir='/kaggle/working/')\n#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n#     subprocess.run(command.split())\n#     nltk.data.path.append('/kaggle/working/')\n\nfrom nltk.corpus import wordnet as wn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nnltk.download('wordnet')\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n    \"\"\"\n     this function create the embedding matrix save in numpy array\n    :param word_index: a dictionary with word: index_value\n    :param embedding_dict: a dict with word embedding\n    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n    :return a numpy array with embedding vectors for all known words\n    \"\"\"\n    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n    ## loop over all the words\n    for word, index in word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[index] = embedding_dict[word]\n    return embedding_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(df_metrics):\n    fig, axs = plt.subplots(3, 2, figsize=(15, 15))  # Creating a grid of subplots\n    fig.suptitle('Training and Testing Metrics Over Epochs')\n\n    # Plotting Train and Test Loss\n    axs[0, 0].plot(df_metrics['Epoch'], df_metrics['Train Loss'], label='Train Loss')\n    axs[0, 0].plot(df_metrics['Epoch'], df_metrics['Test Loss'], label='Test Loss', linestyle='--')\n    axs[0, 0].set_title('Loss')\n    axs[0, 0].set_xlabel('Epoch')\n    axs[0, 0].set_ylabel('Loss')\n    axs[0, 0].legend()\n\n    # Plotting Train and Test Accuracy\n    axs[0, 1].plot(df_metrics['Epoch'], df_metrics['Train Accuracy'], label='Train Accuracy')\n    axs[0, 1].plot(df_metrics['Epoch'], df_metrics['Test Accuracy'], label='Test Accuracy', linestyle='--')\n    axs[0, 1].set_title('Accuracy')\n    axs[0, 1].set_xlabel('Epoch')\n    axs[0, 1].set_ylabel('Accuracy')\n    axs[0, 1].legend()\n\n    # Plotting Train and Test Precision\n    axs[1, 0].plot(df_metrics['Epoch'], df_metrics['Train Precision'], label='Train Precision')\n    axs[1, 0].plot(df_metrics['Epoch'], df_metrics['Test Precision'], label='Test Precision', linestyle='--')\n    axs[1, 0].set_title('Precision')\n    axs[1, 0].set_xlabel('Epoch')\n    axs[1, 0].set_ylabel('Precision')\n    axs[1, 0].legend()\n\n    # Plotting Train and Test Recall\n    axs[1, 1].plot(df_metrics['Epoch'], df_metrics['Train Recall'], label='Train Recall')\n    axs[1, 1].plot(df_metrics['Epoch'], df_metrics['Test Recall'], label='Test Recall', linestyle='--')\n    axs[1, 1].set_title('Recall')\n    axs[1, 1].set_xlabel('Epoch')\n    axs[1, 1].set_ylabel('Recall')\n    axs[1, 1].legend()\n\n    # Plotting Train and Test F1 Score\n    axs[2, 0].plot(df_metrics['Epoch'], df_metrics['Train F1 Score'], label='Train F1 Score')\n    axs[2, 0].plot(df_metrics['Epoch'], df_metrics['Test F1 Score'], label='Test F1 Score', linestyle='--')\n    axs[2, 0].set_title('F1 Score')\n    axs[2, 0].set_xlabel('Epoch')\n    axs[2, 0].set_ylabel('F1 Score')\n    axs[2, 0].legend()\n\n    # Plotting Epoch Time\n    axs[2, 1].plot(df_metrics['Epoch'], df_metrics['Epoch Time'], label='Epoch Time')\n    axs[2, 1].set_title('Time per Epoch')\n    axs[2, 1].set_xlabel('Epoch')\n    axs[2, 1].set_ylabel('Time (sec)')\n    axs[2, 1].legend()\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Initial preprocessing to remove special characters and tokenize\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer()\n    \n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    tokens = word_tokenize(text)\n    tokens = [token for token in tokens if re.match(r'\\b\\w+\\b', token) and token.lower() not in stop_words]\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    tokens = [stemmer.stem(token) for token in tokens]\n    return ' '.join(tokens)\n\ndef tokenize_data(train_texts, test_texts, max_len=25):\n    # Preprocess texts\n    processed_train_texts = [preprocess_text(text) for text in train_texts]\n    processed_test_texts = [preprocess_text(text) for text in test_texts]\n\n    # Create and fit tokenizer on the training texts\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(processed_train_texts)\n    \n    # Convert texts to sequences and pad them\n    train_sequences = tokenizer.texts_to_sequences(processed_train_texts)\n    test_sequences = tokenizer.texts_to_sequences(processed_test_texts)\n    padded_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)\n    padded_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_len)\n    \n    return padded_train_sequences, padded_test_sequences, tokenizer\n\ndfTest = pd.read_csv(\"/kaggle/input/info-retreval/test_pos_neg.csv\", encoding = 'ISO-8859-1').rename(columns={\"Sentiment\":\"Label\"})\ndfTrain = pd.read_csv(\"/kaggle/input/info-retreval/train_pos_neg.csv\", encoding = 'ISO-8859-1').rename(columns={\"Sentiment\":\"Label\"})\ndfTrain[\"Label\"]=dfTrain[\"Label\"].map({\"POSITIVE\":1, \"NEGATIVE\":0}).sample(frac=1).reset_index(drop=True)\ndfTest[\"Label\"]=dfTest[\"Label\"].map({\"POSITIVE\":1, \"NEGATIVE\":0}).sample(frac=1).reset_index(drop=True)\n\n\nglove = pd.read_csv('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove_embedding = {key: val.values for key, val in glove.T.items()}\n\nMAX_LEN = 25\ntrain_texts = dfTrain.Headline.values.tolist() \ntest_texts = dfTest.Headline.values.tolist()\n\nX_train, X_test, tokenizer = tokenize_data(train_texts, test_texts)\nembedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\nbatch_size = 128\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.long)\nX_train_labels_tensor = torch.tensor(dfTrain[\"Label\"], dtype=torch.long)\ntrain_dataset = TensorDataset(X_train_tensor, X_train_labels_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=3)\nX_test_tensor = torch.tensor(X_test, dtype=torch.long)\nX_test_labels_tensor = torch.tensor(dfTest[\"Label\"], dtype=torch.long)\ntest_dataset = TensorDataset(X_test_tensor, X_test_labels_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, num_workers=3)\nembedding_matrix_tensor = torch.tensor(embedding_matrix, dtype=torch.float).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_target(target, seq_lengths, washout, batch_first=False):\n    \"\"\" Preprocess target for offline training.\n\n    Args:\n        target (seq_len, batch, output_size): tensor containing\n            the features of the target sequence.\n        seq_lengths: list of lengths of each sequence in the batch.\n        washout: number of initial timesteps during which output of the\n            reservoir is not forwarded to the readout. One value per sample.\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``\n\n    Returns:\n        tensor containing the features of the batch's sequences rolled out along\n        one axis, minus the washouts and the padded values.\n    \"\"\"\n\n    if batch_first:\n        target = target.transpose(0, 1)\n    n_sequences = target.size(1)\n    target_dim = target.size(2)\n    train_len = sum(torch.tensor(seq_lengths) - torch.tensor(washout)).item()\n\n    new_target = torch.zeros(train_len, target_dim, device=target.device)\n\n    idx = 0\n    for s in range(n_sequences):\n        batch_len = seq_lengths[s] - washout[s]\n        new_target[idx:idx + batch_len, :] = target[washout[s]:seq_lengths[s], s, :]\n        idx += batch_len\n\n    return new_target\n\n\ndef washout_tensor(tensor, washout, seq_lengths, bidirectional=False, batch_first=False):\n    tensor = tensor.transpose(0, 1) if batch_first else tensor.clone()\n    if type(seq_lengths) == list:\n        seq_lengths = seq_lengths.copy()\n    if type(seq_lengths) == torch.Tensor:\n        seq_lengths = seq_lengths.clone()\n        \n    \n    for b in range(tensor.size(1)):\n        if washout[b] > 0:\n            tmp = tensor[washout[b]:seq_lengths[b], b].clone()\n            tensor[:seq_lengths[b] - washout[b], b] = tmp\n            tensor[seq_lengths[b] - washout[b]:, b] = 0\n            seq_lengths[b] -= washout[b]\n\n            if bidirectional:\n                tensor[seq_lengths[b] - washout[b]:, b] = 0\n                seq_lengths[b] -= washout[b]\n\n    if type(seq_lengths) == list:\n        max_len = max(seq_lengths)\n    else:\n        max_len = max(seq_lengths).item()\n\n    return tensor[:max_len], seq_lengths\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import PackedSequence, pad_packed_sequence\n\n\n\nclass ESN(nn.Module):\n    \"\"\" Applies an Echo State Network to an input sequence. Multi-layer Echo\n    State Network is based on paper\n    Deep Echo State Network (DeepESN): A Brief Survey - Gallicchio, Micheli 2017\n\n    Args:\n        input_size: The number of expected features in the input x.\n        hidden_size: The number of features in the hidden state h.\n        output_size: The number of expected features in the output y.\n        num_layers: Number of recurrent layers. Default: 1\n        nonlinearity: The non-linearity to use ['tanh'|'relu'|'id'].\n            Default: 'tanh'\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``\n        leaking_rate: Leaking rate of reservoir's neurons. Default: 1\n        spectral_radius: Desired spectral radius of recurrent weight matrix.\n            Default: 0.9\n        w_ih_scale: Scale factor for first layer's input weights (w_ih_l0). It\n            can be a number or a tensor of size '1 + input_size' and first element\n            is the bias' scale factor. Default: 1\n        lambda_reg: Ridge regression's shrinkage parameter. Default: 1\n        density: Recurrent weight matrix's density. Default: 1\n        w_io: If 'True', then the network uses trainable input-to-output\n            connections. Default: ``False``\n        readout_training: Readout's traning algorithm ['gd'|'svd'|'cholesky'|'inv'].\n            If 'gd', gradients are accumulated during backward\n            pass. If 'svd', 'cholesky' or 'inv', the network will learn readout's\n            parameters during the forward pass using ridge regression. The\n            coefficients are computed using SVD, Cholesky decomposition or\n            standard ridge regression formula. 'gd', 'cholesky' and 'inv'\n            permit the usage of mini-batches to train the readout.\n            If 'inv' and matrix is singular, pseudoinverse is used.\n        output_steps: defines how the reservoir's output will be used by ridge\n            regression method ['all', 'mean', 'last'].\n            If 'all', the entire reservoir output matrix will be used.\n            If 'mean', the mean of reservoir output matrix along the timesteps\n            dimension will be used.\n            If 'last', only the last timestep of the reservoir output matrix\n            will be used.\n            'mean' and 'last' are useful for classification tasks.\n\n    Inputs: input, washout, h_0, target\n        input (seq_len, batch, input_size): tensor containing the features of\n            the input sequence. The input can also be a packed variable length\n            sequence. See `torch.nn.utils.rnn.pack_padded_sequence`\n        washout (batch): number of initial timesteps during which output of the\n            reservoir is not forwarded to the readout. One value per batch's\n            sample.\n        h_0 (num_layers, batch, hidden_size): tensor containing\n             the initial reservoir's hidden state for each element in the batch.\n             Defaults to zero if not provided.\n\n        target (seq_len*batch - washout*batch, output_size): tensor containing\n            the features of the batch's target sequences rolled out along one\n            axis, minus the washouts and the padded values. It is only needed\n            for readout's training in offline mode. Use `prepare_target` to\n            compute it.\n\n    Outputs: output, h_n\n        - output (seq_len, batch, hidden_size): tensor containing the output\n        features (h_k) from the readout, for each k.\n        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n          containing the reservoir's hidden state for k=seq_len.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1,\n                 nonlinearity='tanh', batch_first=False, leaking_rate=1,\n                 spectral_radius=0.9, w_ih_scale=1, lambda_reg=0, density=1,\n                 w_io=False, readout_training='svd', output_steps='all'):\n        super(ESN, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        if nonlinearity == 'tanh':\n            mode = 'RES_TANH'\n        elif nonlinearity == 'relu':\n            mode = 'RES_RELU'\n        elif nonlinearity == 'id':\n            mode = 'RES_ID'\n        else:\n            raise ValueError(\"Unknown nonlinearity '{}'\".format(nonlinearity))\n        self.batch_first = batch_first\n        self.leaking_rate = leaking_rate\n        self.spectral_radius = spectral_radius\n        if type(w_ih_scale) != torch.Tensor:\n            self.w_ih_scale = torch.ones(input_size + 1)\n            self.w_ih_scale *= w_ih_scale\n        else:\n            self.w_ih_scale = w_ih_scale\n\n        self.lambda_reg = lambda_reg\n        self.density = density\n        self.w_io = w_io\n        if readout_training in {'gd', 'svd', 'cholesky', 'inv'}:\n            self.readout_training = readout_training\n        else:\n            raise ValueError(\"Unknown readout training algorithm '{}'\".format(\n                readout_training))\n\n        self.reservoir = Reservoir(mode, input_size, hidden_size, num_layers,\n                                   leaking_rate, spectral_radius,\n                                   self.w_ih_scale, density,\n                                   batch_first=batch_first)\n\n        if w_io:\n            self.readout = nn.Linear(input_size + hidden_size * num_layers,\n                                     output_size)\n        else:\n            self.readout = nn.Linear(hidden_size * num_layers, output_size)\n        if readout_training == 'offline':\n            self.readout.weight.requires_grad = False\n\n        if output_steps in {'all', 'mean', 'last'}:\n            self.output_steps = output_steps\n        else:\n            raise ValueError(\"Unknown task '{}'\".format(\n                output_steps))\n\n        self.XTX = None\n        self.XTy = None\n        self.X = None\n\n    def forward(self, input, washout, h_0=None, target=None):\n        with torch.no_grad():\n            is_packed = isinstance(input, PackedSequence)\n\n            output, hidden = self.reservoir(input, h_0)\n            if is_packed:\n                output, seq_lengths = pad_packed_sequence(output,\n                                                          batch_first=self.batch_first)\n            else:\n                if self.batch_first:\n                    seq_lengths = output.size(0) * [output.size(1)]\n                else:\n                    seq_lengths = output.size(1) * [output.size(0)]\n\n            if self.batch_first:\n                output = output.transpose(0, 1)\n                \n\n            output, seq_lengths = washout_tensor(output, washout, seq_lengths)\n\n            if self.w_io:\n                if is_packed:\n                    input, input_lengths = pad_packed_sequence(input,\n                                                          batch_first=self.batch_first)\n                else:\n                    input_lengths = [input.size(0)] * input.size(1)\n\n                if self.batch_first:\n                    input = input.transpose(0, 1)\n\n                input, _ = washout_tensor(input, washout, input_lengths)\n\n                output = torch.cat([input, output], -1)\n\n            if self.readout_training == 'gd' or target is None:\n                with torch.enable_grad():\n                    output = self.readout(output)\n\n                    if is_packed:\n                        for i in range(output.size(1)):\n                            if seq_lengths[i] < output.size(0):\n                                output[seq_lengths[i]:, i] = 0\n\n                    if self.batch_first:\n                        output = output.transpose(0, 1)\n\n                    # Uncomment if you want packed output.\n                    # if is_packed:\n                    #     output = pack_padded_sequence(output, seq_lengths,\n                    #                                   batch_first=self.batch_first)\n\n                    return output, hidden\n\n            else:\n                batch_size = output.size(1)\n\n                X = torch.ones(target.size(0), 1 + output.size(2), device=target.device)\n                row = 0\n                for s in range(batch_size):\n                    if self.output_steps == 'all':\n                        X[row:row + seq_lengths[s], 1:] = output[:seq_lengths[s],\n                                                          s]\n                        row += seq_lengths[s]\n                    elif self.output_steps == 'mean':\n                        X[row, 1:] = torch.mean(output[:seq_lengths[s], s], 0)\n                        row += 1\n                    elif self.output_steps == 'last':\n                        X[row, 1:] = output[seq_lengths[s] - 1, s]\n                        row += 1\n\n                if self.readout_training == 'cholesky':\n                    if self.XTX is None:\n                        self.XTX = torch.mm(X.t(), X)\n                        self.XTy = torch.mm(X.t(), target)\n                    else:\n                        self.XTX += torch.mm(X.t(), X)\n                        self.XTy += torch.mm(X.t(), target)\n\n                elif self.readout_training == 'svd':\n                    # Scikit-Learn SVD solver for ridge regression.\n                    U, s, V = torch.svd(X)\n                    idx = s > 1e-15  # same default value as scipy.linalg.pinv\n                    s_nnz = s[idx][:, None]\n                    UTy = torch.mm(U.t(), target)\n                    d = torch.zeros(s.size(0), 1, device=X.device)\n                    d[idx] = s_nnz / (s_nnz ** 2 + self.lambda_reg)\n                    d_UT_y = d * UTy\n                    W = torch.mm(V, d_UT_y).t()\n\n                    self.readout.bias = nn.Parameter(W[:, 0])\n                    self.readout.weight = nn.Parameter(W[:, 1:])\n                elif self.readout_training == 'inv':\n                    self.X = X\n                    if self.XTX is None:\n                        self.XTX = torch.mm(X.t(), X)\n                        self.XTy = torch.mm(X.t(), target)\n                    else:\n                        self.XTX += torch.mm(X.t(), X)\n                        self.XTy += torch.mm(X.t(), target)\n\n                return None, None\n\n    def fit(self):\n        if self.readout_training in {'gd', 'svd'}:\n            return\n\n        if self.readout_training == 'cholesky':\n            W = torch.linalg.solve(self.XTy,\n                                   self.XTX + self.lambda_reg * torch.eye(\n                                       self.XTX.size(0), device=self.XTX.device))[0].t()\n            self.XTX = None\n            self.XTy = None\n\n            self.readout.bias = nn.Parameter(W[:, 0])\n            self.readout.weight = nn.Parameter(W[:, 1:])\n        elif self.readout_training == 'inv':\n            I = (self.lambda_reg * torch.eye(self.XTX.size(0))).to(\n                self.XTX.device)\n            A = self.XTX + I\n            X_rank = torch.linalg.matrix_rank(A).item()\n\n            if X_rank == self.X.size(0):\n                W = torch.mm(torch.inverse(A), self.XTy).t()\n            else:\n                W = torch.mm(torch.pinverse(A), self.XTy).t()\n\n            self.readout.bias = nn.Parameter(W[:, 0])\n            self.readout.weight = nn.Parameter(W[:, 1:])\n\n            self.XTX = None\n            self.XTy = None\n\n    def reset_parameters(self):\n        self.reservoir.reset_parameters()\n        self.readout.reset_parameters()\n\n        \n        \nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import PackedSequence\nimport torch.sparse\nimport re\n\n\ndef apply_permutation(tensor, permutation, dim=1):\n    # type: (Tensor, Tensor, int) -> Tensor\n    return tensor.index_select(dim, permutation)\n\n\nclass Reservoir(nn.Module):\n\n    def __init__(self, mode, input_size, hidden_size, num_layers, leaking_rate,\n                 spectral_radius, w_ih_scale,\n                 density, bias=True, batch_first=False):\n        super(Reservoir, self).__init__()\n        self.mode = mode\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.leaking_rate = leaking_rate\n        self.spectral_radius = spectral_radius\n        self.w_ih_scale = w_ih_scale\n        self.density = density\n        self.bias = bias\n        self.batch_first = batch_first\n\n        self._all_weights = []\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n\n            w_ih = nn.Parameter(torch.Tensor(hidden_size, layer_input_size))\n            w_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n            b_ih = nn.Parameter(torch.Tensor(hidden_size))\n            layer_params = (w_ih, w_hh, b_ih)\n\n            param_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']\n            if bias:\n                param_names += ['bias_ih_l{}{}']\n            param_names = [x.format(layer, '') for x in param_names]\n\n            for name, param in zip(param_names, layer_params):\n                setattr(self, name, param)\n            self._all_weights.append(param_names)\n\n        self.reset_parameters()\n\n    def _apply(self, fn):\n        ret = super(Reservoir, self)._apply(fn)\n        return ret\n\n    def reset_parameters(self):\n        weight_dict = self.state_dict()\n        for key, value in weight_dict.items():\n            if key == 'weight_ih_l0':\n                nn.init.uniform_(value, -1, 1)\n                value *= self.w_ih_scale[1:]\n            elif re.fullmatch('weight_ih_l[^0]*', key):\n                nn.init.uniform_(value, -1, 1)\n            elif re.fullmatch('bias_ih_l[0-9]*', key):\n                nn.init.uniform_(value, -1, 1)\n                value *= self.w_ih_scale[0]\n            elif re.fullmatch('weight_hh_l[0-9]*', key):\n                w_hh = torch.Tensor(self.hidden_size * self.hidden_size)\n                w_hh.uniform_(-1, 1)\n                if self.density < 1:\n                    zero_weights = torch.randperm(\n                        int(self.hidden_size * self.hidden_size))\n                    zero_weights = zero_weights[\n                                   :int(\n                                       self.hidden_size * self.hidden_size * (\n                                                   1 - self.density))]\n                    w_hh[zero_weights] = 0\n                w_hh = w_hh.view(self.hidden_size, self.hidden_size)\n                abs_eigs = torch.abs(torch.linalg.eigvals(w_hh))\n                weight_dict[key] = w_hh * (self.spectral_radius / torch.max(abs_eigs))\n\n        self.load_state_dict(weight_dict)\n\n    def check_input(self, input, batch_sizes):\n        # type: (Tensor, Optional[Tensor]) -> None\n        expected_input_dim = 2 if batch_sizes is not None else 3\n        if input.dim() != expected_input_dim:\n            raise RuntimeError(\n                'input must have {} dimensions, got {}'.format(\n                    expected_input_dim, input.dim()))\n        if self.input_size != input.size(-1):\n            raise RuntimeError(\n                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n                    self.input_size, input.size(-1)))\n\n    def get_expected_hidden_size(self, input, batch_sizes):\n        # type: (Tensor, Optional[Tensor]) -> Tuple[int, int, int]\n        if batch_sizes is not None:\n            mini_batch = batch_sizes[0]\n            mini_batch = int(mini_batch)\n        else:\n            mini_batch = input.size(0) if self.batch_first else input.size(1)\n        expected_hidden_size = (self.num_layers, mini_batch, self.hidden_size)\n        return expected_hidden_size\n\n    def check_hidden_size(self, hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n        # type: (Tensor, Tuple[int, int, int], str) -> None\n        if hx.size() != expected_hidden_size:\n            raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n\n    def check_forward_args(self, input, hidden, batch_sizes):\n        # type: (Tensor, Tensor, Optional[Tensor]) -> None\n        self.check_input(input, batch_sizes)\n        expected_hidden_size = self.get_expected_hidden_size(input, batch_sizes)\n\n        self.check_hidden_size(hidden, expected_hidden_size)\n\n    def permute_hidden(self, hx, permutation):\n        # type: (Tensor, Optional[Tensor]) -> Tensor\n        if permutation is None:\n            return hx\n        return apply_permutation(hx, permutation)\n\n    def forward(self, input, hx=None):\n        is_packed = isinstance(input, PackedSequence)\n        if is_packed:\n            input, batch_sizes, sorted_indices, unsorted_indices = input\n            max_batch_size = int(batch_sizes[0])\n        else:\n            batch_sizes = None\n            max_batch_size = input.size(0) if self.batch_first else input.size(1)\n            sorted_indices = None\n            unsorted_indices = None\n\n        if hx is None:\n            hx = input.new_zeros(self.num_layers, max_batch_size,\n                                 self.hidden_size, requires_grad=False)\n        else:\n            # Each batch of the hidden state should match the input sequence that\n            # the user believes he/she is passing in.\n            hx = self.permute_hidden(hx, sorted_indices)\n\n        flat_weight = None\n\n        self.check_forward_args(input, hx, batch_sizes)\n        func = AutogradReservoir(\n            self.mode,\n            self.input_size,\n            self.hidden_size,\n            num_layers=self.num_layers,\n            batch_first=self.batch_first,\n            train=self.training,\n            variable_length=is_packed,\n            flat_weight=flat_weight,\n            leaking_rate=self.leaking_rate\n        )\n        output, hidden = func(input, self.all_weights, hx, batch_sizes)\n        if is_packed:\n            output = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)\n        return output, self.permute_hidden(hidden, unsorted_indices)\n\n    def extra_repr(self):\n        s = '({input_size}, {hidden_size}'\n        if self.num_layers != 1:\n            s += ', num_layers={num_layers}'\n        if self.bias is not True:\n            s += ', bias={bias}'\n        if self.batch_first is not False:\n            s += ', batch_first={batch_first}'\n        s += ')'\n        return s.format(**self.__dict__)\n\n    def __setstate__(self, d):\n        super(Reservoir, self).__setstate__(d)\n        self.__dict__.setdefault('_data_ptrs', [])\n        if 'all_weights' in d:\n            self._all_weights = d['all_weights']\n        if isinstance(self._all_weights[0][0], str):\n            return\n        num_layers = self.num_layers\n        self._all_weights = []\n        for layer in range(num_layers):\n            weights = ['weight_ih_l{}{}', 'weight_hh_l{}{}', 'bias_ih_l{}{}']\n            weights = [x.format(layer) for x in weights]\n            if self.bias:\n                self._all_weights += [weights]\n            else:\n                self._all_weights += [weights[:2]]\n\n    @property\n    def all_weights(self):\n        return [[getattr(self, weight) for weight in weights] for weights in\n                self._all_weights]\n\n\ndef StackedRNN(inners, num_layers, lstm=False, train=True):\n    num_directions = len(inners)\n    total_layers = num_layers * num_directions\n\n    def forward(input, hidden, weight, batch_sizes):\n        assert (len(weight) == total_layers)\n        next_hidden = []\n        all_layers_output = []\n\n        for i in range(num_layers):\n            all_output = []\n            for j, inner in enumerate(inners):\n                l = i * num_directions + j\n\n                hy, output = inner(input, hidden[l], weight[l], batch_sizes)\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n            all_layers_output.append(input)\n\n        all_layers_output = torch.cat(all_layers_output, -1)\n        next_hidden = torch.cat(next_hidden, 0).view(\n            total_layers, *next_hidden[0].size())\n\n        return next_hidden, all_layers_output\n\n    return forward\n\ndef AutogradReservoir(mode, input_size, hidden_size, num_layers=1,\n                      batch_first=False, train=True,\n                      batch_sizes=None, variable_length=False, flat_weight=None,\n                      leaking_rate=1):\n    if mode == 'RES_TANH':\n        cell = ResTanhCell\n    elif mode == 'RES_RELU':\n        cell = ResReLUCell\n    elif mode == 'RES_ID':\n        cell = ResIdCell\n\n    if variable_length:\n        layer = (VariableRecurrent(cell, leaking_rate),)\n    else:\n        layer = (Recurrent(cell, leaking_rate),)\n\n    func = StackedRNN(layer,\n                      num_layers,\n                      False,\n                      train=train)\n\n    def forward(input, weight, hidden, batch_sizes):\n        if batch_first and batch_sizes is None:\n            input = input.transpose(0, 1)\n\n        nexth, output = func(input, hidden, weight, batch_sizes)\n\n        if batch_first and not variable_length:\n            output = output.transpose(0, 1)\n\n        return output, nexth\n\n    return forward\n\n\ndef Recurrent(inner, leaking_rate):\n    def forward(input, hidden, weight, batch_sizes):\n        output = []\n        steps = range(input.size(0))\n        for i in steps:\n            hidden = inner(input[i], hidden, leaking_rate, *weight)\n            # hack to handle LSTM\n            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n\n        return hidden, output\n\n    return forward\n\n\ndef VariableRecurrent(inner, leaking_rate):\n    def forward(input, hidden, weight, batch_sizes):\n        output = []\n        input_offset = 0\n        last_batch_size = batch_sizes[0]\n        hiddens = []\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n        for batch_size in batch_sizes:\n            step_input = input[input_offset:input_offset + batch_size]\n            input_offset += batch_size\n\n            dec = last_batch_size - batch_size\n            if dec > 0:\n                hiddens.append(tuple(h[-dec:] for h in hidden))\n                hidden = tuple(h[:-dec] for h in hidden)\n            last_batch_size = batch_size\n\n            if flat_hidden:\n                hidden = (inner(step_input, hidden[0], leaking_rate, *weight),)\n            else:\n                hidden = inner(step_input, hidden, leaking_rate, *weight)\n\n            output.append(hidden[0])\n        hiddens.append(hidden)\n        hiddens.reverse()\n\n        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n        assert hidden[0].size(0) == batch_sizes[0]\n        if flat_hidden:\n            hidden = hidden[0]\n        output = torch.cat(output, 0)\n\n        return hidden, output\n\n    return forward\n\n\n    num_directions = len(inners)\n    total_layers = num_layers * num_directions\n\n    def forward(input, hidden, weight, batch_sizes):\n        assert (len(weight) == total_layers)\n        next_hidden = []\n        all_layers_output = []\n\n        for i in range(num_layers):\n            all_output = []\n            for j, inner in enumerate(inners):\n                l = i * num_directions + j\n\n                hy, output = inner(input, hidden[l], weight[l], batch_sizes)\n                next_hidden.append(hy)\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n            all_layers_output.append(input)\n\n        all_layers_output = torch.cat(all_layers_output, -1)\n        next_hidden = torch.cat(next_hidden, 0).view(\n            total_layers, *next_hidden[0].size())\n\n        return next_hidden, all_layers_output\n\n    return forward\n\n\ndef ResTanhCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n    hy_ = torch.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh))\n    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n    return hy\n\n\ndef ResReLUCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n    hy_ = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh))\n    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n    return hy\n\n\ndef ResIdCell(input, hidden, leaking_rate, w_ih, w_hh, b_ih=None):\n    hy_ = F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh)\n    hy = (1 - leaking_rate) * hidden + leaking_rate * hy_\n    return hy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_NN_metrics(model, criterion, optimizer, num_epochs, train_loader, test_loader, device, verbose):\n    metrics = {\n        'Epoch': [],\n        'Train Loss': [],\n        'Train Accuracy': [],\n        'Train Precision': [],\n        'Train Recall': [],\n        'Train F1 Score': [],\n        'Test Loss': [],\n        'Test Accuracy': [],\n        'Test Precision': [],\n        'Test Recall': [],\n        'Test F1 Score': [],\n        'Epoch Time': []\n    }\n\n    for epoch in range(num_epochs):  # num_epochs is the number of epochs you want to train for\n        epoch_start_time = time.time()  # Start time of the epoch\n\n        # Training phase\n        model.train()\n        running_loss = 0\n        running_correct = 0\n        total_train = 0\n        all_train_predicted = []\n        all_train_labels = []\n\n        for inputs, labels in train_loader:  # train_loader is your DataLoader for training data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()  # Zero the gradients\n            outputs = model(inputs)  # Get model predictions\n            loss = criterion(outputs, labels)  # Calculate loss\n            loss.backward()  # Backpropagate the loss\n            optimizer.step()  # Update model parameters\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            running_correct += (predicted == labels).sum().item()\n            total_train += labels.size(0)\n            all_train_predicted.extend(predicted.cpu().numpy())\n            all_train_labels.extend(labels.cpu().numpy())\n            \n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        train_loss = running_loss / total_train\n        train_accuracy = running_correct / total_train\n        train_precision = precision_score(all_train_labels, all_train_predicted, average='macro')\n        train_recall = recall_score(all_train_labels, all_train_predicted, average='macro')\n        train_f1 = f1_score(all_train_labels, all_train_predicted, average='macro')\n        \n        # Testing phase\n        model.eval()\n        running_loss = 0\n        running_correct = 0\n        total_test = 0\n        all_test_predicted = []\n        all_test_labels = []\n\n        with torch.no_grad():  # No need to calculate gradients during testing\n            for inputs, labels in test_loader:  # test_loader is your DataLoader for testing data\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                running_correct += (predicted == labels).sum().item()\n                total_test += labels.size(0)\n                all_test_predicted.extend(predicted.cpu().numpy())\n                all_test_labels.extend(labels.cpu().numpy())\n\n        test_loss = running_loss / total_test\n        test_accuracy = running_correct / total_test\n        test_precision = precision_score(all_test_labels, all_test_predicted, average='macro')\n        test_recall = recall_score(all_test_labels, all_test_predicted, average='macro')\n        test_f1 = f1_score(all_test_labels, all_test_predicted, average='macro')\n        \n\n        # Append metrics for this epoch to the dictionary\n        metrics['Epoch'].append(epoch + 1)\n        metrics['Train Loss'].append(train_loss)\n        metrics['Train Accuracy'].append(train_accuracy)\n        metrics['Train Precision'].append(train_precision)\n        metrics['Train Recall'].append(train_recall)\n        metrics['Train F1 Score'].append(train_f1)\n        metrics['Test Loss'].append(test_loss)\n        metrics['Test Accuracy'].append(test_accuracy)\n        metrics['Test Precision'].append(test_precision)\n        metrics['Test Recall'].append(test_recall)\n        metrics['Test F1 Score'].append(test_f1)\n        metrics['Epoch Time'].append(epoch_duration)\n        if verbose:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%, \"\n                  f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1 Score: {train_f1:.4f}, \"\n                  f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%, Test Precision: {test_precision:.4f}, \"\n                  f\"Test Recall: {test_recall:.4f}, Test F1 Score: {test_f1:.4f}, Epoch Time: {epoch_duration:.2f} sec\")\n\n    df_metrics = pd.DataFrame(metrics)\n    return df_metrics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextESNClassifier(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size, num_layers, num_classes, washout, nonlinearity='tanh', leaking_rate=1.0, spectral_radius=0.9, w_ih_scale=1.0, lambda_reg=1.0, density=1.0):\n        super(TextESNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True).to(device)\n        self.esn = ESN(input_size=embedding_matrix.size(1), hidden_size=hidden_size, num_layers=num_layers, w_io=True, output_size=hidden_size, output_steps='mean', batch_first=False, nonlinearity=nonlinearity, leaking_rate=leaking_rate, spectral_radius=spectral_radius, w_ih_scale=w_ih_scale, lambda_reg=lambda_reg, density=density)\n        self.output_layer = nn.Linear(hidden_size, num_classes)\n        self.washout = washout  # Set the washout as a fixed attribute, or make it adjustable as per requirement\n\n    def forward(self, x, h_0=None):\n        # Transpose x from [batch_size, seq_len, input_size] to [seq_len, batch_size, input_size]\n\n        # Proceed with embedding if applicable\n        x = self.embedding(x)\n        x = x.transpose(0, 1)  # Now x is of shape [seq_len, batch_size, input_size]\n\n        # Pass through ESN\n        x, _ = self.esn(x, washout=torch.full((x.size(1),), self.washout, dtype=torch.long, device=x.device))  # ESN should be defined with batch_first=False\n\n        # You might need to adjust how you handle outputs depending on your specific model and task\n        x = x[-1, :, :]  # Now x is of shape [batch_size, hidden_size]\n\n        # Pass through the output layer\n        logits = self.output_layer(x)\n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(df_metrics):\n    fig, axs = plt.subplots(3, 2, figsize=(15, 15))  # Creating a grid of subplots\n    fig.suptitle('Training and Testing Metrics Over Epochs')\n\n    # Plotting Train and Test Loss\n    axs[0, 0].plot(df_metrics['Epoch'], df_metrics['Train Loss'], label='Train Loss')\n    axs[0, 0].plot(df_metrics['Epoch'], df_metrics['Test Loss'], label='Test Loss', linestyle='--')\n    axs[0, 0].set_title('Loss')\n    axs[0, 0].set_xlabel('Epoch')\n    axs[0, 0].set_ylabel('Loss')\n    axs[0, 0].legend()\n\n    # Plotting Train and Test Accuracy\n    axs[0, 1].plot(df_metrics['Epoch'], df_metrics['Train Accuracy'], label='Train Accuracy')\n    axs[0, 1].plot(df_metrics['Epoch'], df_metrics['Test Accuracy'], label='Test Accuracy', linestyle='--')\n    axs[0, 1].set_title('Accuracy')\n    axs[0, 1].set_xlabel('Epoch')\n    axs[0, 1].set_ylabel('Accuracy')\n    axs[0, 1].legend()\n\n    # Plotting Train and Test Precision\n    axs[1, 0].plot(df_metrics['Epoch'], df_metrics['Train Precision'], label='Train Precision')\n    axs[1, 0].plot(df_metrics['Epoch'], df_metrics['Test Precision'], label='Test Precision', linestyle='--')\n    axs[1, 0].set_title('Precision')\n    axs[1, 0].set_xlabel('Epoch')\n    axs[1, 0].set_ylabel('Precision')\n    axs[1, 0].legend()\n\n    # Plotting Train and Test Recall\n    axs[1, 1].plot(df_metrics['Epoch'], df_metrics['Train Recall'], label='Train Recall')\n    axs[1, 1].plot(df_metrics['Epoch'], df_metrics['Test Recall'], label='Test Recall', linestyle='--')\n    axs[1, 1].set_title('Recall')\n    axs[1, 1].set_xlabel('Epoch')\n    axs[1, 1].set_ylabel('Recall')\n    axs[1, 1].legend()\n\n    # Plotting Train and Test F1 Score\n    axs[2, 0].plot(df_metrics['Epoch'], df_metrics['Train F1 Score'], label='Train F1 Score')\n    axs[2, 0].plot(df_metrics['Epoch'], df_metrics['Test F1 Score'], label='Test F1 Score', linestyle='--')\n    axs[2, 0].set_title('F1 Score')\n    axs[2, 0].set_xlabel('Epoch')\n    axs[2, 0].set_ylabel('F1 Score')\n    axs[2, 0].legend()\n\n    # Plotting Epoch Time\n    axs[2, 1].plot(df_metrics['Epoch'], df_metrics['Epoch Time'], label='Epoch Time')\n    axs[2, 1].set_title('Time per Epoch')\n    axs[2, 1].set_xlabel('Epoch')\n    axs[2, 1].set_ylabel('Time (sec)')\n    axs[2, 1].legend()\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TextESNClassifier(embedding_matrix_tensor, hidden_size=128,num_layers=2, num_classes=2, washout=2).to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-2)\nnum_epochs = 100\ndf_metrics = train_test_NN_metrics(model, criterion, optimizer, num_epochs, train_loader, test_loader, device, verbose=False)\nplot_metrics(df_metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics.to_csv(\"TextESNClassifier_pos.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionLayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(AttentionLayer, self).__init__()\n        self.query_layer = nn.Linear(hidden_size, hidden_size)\n        self.key_layer = nn.Linear(hidden_size, hidden_size)\n        self.value_layer = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, x):\n        # x is expected to be of shape [seq_len, batch_size, hidden_size]\n        query = self.query_layer(x)\n        key = self.key_layer(x)\n        value = self.value_layer(x)\n\n        # Compute attention scores\n        scores = torch.bmm(query.transpose(0, 1), key.transpose(0, 1).transpose(1, 2)) / (query.size(2) ** 0.5)\n        attn_weights = F.softmax(scores, dim=-1)\n\n        # Apply attention weights to values\n        attended = torch.bmm(attn_weights, value.transpose(0, 1))\n\n        return attended.transpose(0, 1)  # Return to shape [seq_len, batch_size, hidden_size]\n\nclass ESNClassifier3(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size, num_layers, num_classes, washout):\n        super(ESNClassifier3, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True).to(device)\n        self.esn1 = ESN(input_size=embedding_matrix.size(1), hidden_size=hidden_size, num_layers=num_layers, w_io=True, output_size=hidden_size, output_steps='mean', batch_first=False)\n        self.attention1 = AttentionLayer(hidden_size)\n        self.esn2 = ESN(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, w_io=True, output_size=hidden_size, output_steps='mean', batch_first=False)\n\n        # Additional Attention and ESN layers\n        self.attention2 = AttentionLayer(hidden_size)\n        self.esn3 = ESN(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, w_io=True, output_size=hidden_size, output_steps='mean', batch_first=False)\n\n        self.output_layer = nn.Linear(hidden_size, num_classes)\n        self.washout = washout\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.transpose(0, 1)  # [seq_len, batch_size, input_size]\n\n        # First ESN and Attention layer\n        esn1_output, _ = self.esn1(x, washout=torch.full((x.size(1),), self.washout, dtype=torch.long, device=x.device))\n        attention1_output = self.attention1(esn1_output)\n        combined1_output = esn1_output + attention1_output  # Element-wise addition (skip connection)\n\n        # First ESN's output passed to second ESN\n        esn2_output, _ = self.esn2(combined1_output, washout=torch.full((x.size(1),), self.washout, dtype=torch.long, device=x.device))\n\n        # Additional Attention and ESN layers\n        attention2_output = self.attention2(esn2_output)\n        combined2_output = esn2_output + attention2_output  # Element-wise addition (skip connection)\n        esn3_output, _ = self.esn3(combined2_output, washout=torch.full((x.size(1),), self.washout, dtype=torch.long, device=x.device))\n\n        # Taking the last output of all sequences for the final classification\n        final_output = esn3_output[-1, :, :]\n\n        logits = self.output_layer(final_output)\n        return logits\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ESNClassifier3(embedding_matrix_tensor, hidden_size=128,num_layers=2, num_classes=2, washout=2).to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-2)\nnum_epochs = 100\ndf_metrics = train_test_NN_metrics(model, criterion, optimizer, num_epochs, train_loader, test_loader, device, verbose=False)\nplot_metrics(df_metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics.to_csv(\"TextESNClassifier_pos.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest = pd.read_csv(\"/kaggle/input/info-retreval/test_neu_opi.csv\", encoding = 'ISO-8859-1').rename(columns={\"Sentiment\":\"Label\"})\ndfTrain = pd.read_csv(\"/kaggle/input/info-retreval/train_neu_opi.csv\", encoding = 'ISO-8859-1').rename(columns={\"Sentiment\":\"Label\"})\ndfTrain[\"Label\"]=dfTrain[\"Label\"].map({\"OPINIONATED\":1, \"NEUTRAL\":0}).sample(frac=1).reset_index(drop=True)\ndfTest[\"Label\"]=dfTest[\"Label\"].map({\"OPINIONATED\":1, \"NEUTRAL\":0}).sample(frac=1).reset_index(drop=True)\n\n\nglove = pd.read_csv('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove_embedding = {key: val.values for key, val in glove.T.items()}\n\nMAX_LEN = 25\ntrain_texts = dfTrain.Headline.values.tolist() \ntest_texts = dfTest.Headline.values.tolist()\n\nX_train, X_test, tokenizer = tokenize_data(train_texts, test_texts)\nembedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\nbatch_size = 128\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.long)\nX_train_labels_tensor = torch.tensor(dfTrain[\"Label\"], dtype=torch.long)\ntrain_dataset = TensorDataset(X_train_tensor, X_train_labels_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=3)\nX_test_tensor = torch.tensor(X_test, dtype=torch.long)\nX_test_labels_tensor = torch.tensor(dfTest[\"Label\"], dtype=torch.long)\ntest_dataset = TensorDataset(X_test_tensor, X_test_labels_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, num_workers=3)\nembedding_matrix_tensor = torch.tensor(embedding_matrix, dtype=torch.float).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TextESNClassifier(embedding_matrix_tensor, hidden_size=128,num_layers=2, num_classes=2, washout=2).to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-2)\nnum_epochs = 100\ndf_metrics = train_test_NN_metrics(model, criterion, optimizer, num_epochs, train_loader, test_loader, device, verbose=False)\nplot_metrics(df_metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics.to_csv(\"TextESNClassifier_neu.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ESNClassifier3(embedding_matrix_tensor, hidden_size=128,num_layers=2, num_classes=2, washout=2).to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-2)\nnum_epochs = 100\ndf_metrics = train_test_NN_metrics(model, criterion, optimizer, num_epochs, train_loader, test_loader, device, verbose=False)\nplot_metrics(df_metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics.to_csv(\"TextESNClassifier3_neu.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}